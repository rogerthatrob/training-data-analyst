{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2020 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "### Overview\n",
    "\n",
    "In this notebook, you'll learn how to submit a job to AI Platform Training. In the job you'll train your TensorFlow 2 model and export the saved model to Cloud Storage.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "[CTA - Ridership - Daily Boarding Totals](https://data.cityofchicago.org/Transportation/CTA-Ridership-Daily-Boarding-Totals/6iiy-9s97): This dataset shows systemwide boardings for both bus and rail services provided by Chicago Transit Authority, dating back to 2001.\n",
    "\n",
    "### Objective\n",
    "\n",
    "The goal is to forecast future transit ridership in the City of Chicago, based on previous ridership."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "## Install packages and dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "colab_type": "code",
    "id": "kRv5imUnKkuP",
    "outputId": "f2f8b527-1b8b-4d7b-c69d-dc9426f62cf6"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import googleapiclient.discovery\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from google.api_core.client_options import ClientOptions\n",
    "from google.cloud import storage\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the TensorFlow version installed\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oM1iC_MfAts1"
   },
   "outputs": [],
   "source": [
    "# Enter your project, region, and a bucket name. Then run the  cell to make sure the\n",
    "# Cloud SDK uses the right project for all the commands in this notebook.\n",
    "\n",
    "PROJECT = 'your-project-name' # REPLACE WITH YOUR PROJECT ID\n",
    "BUCKET = 'your-bucket-name' # REPLACE WITH A UNIQUE BUCKET NAME e.g. your PROJECT NAME\n",
    "REGION = 'us-central1' # REPLACE WITH YOUR BUCKET REGION e.g. us-central1\n",
    "BUCKET_URI = 'gs://' + BUCKET\n",
    "\n",
    "#Don't change the following command - this is to check if you have changed the project name above.\n",
    "assert PROJECT != 'your-project-name', 'Don''t forget to change the project variables!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset parameters\n",
    "\n",
    "target_col = 'total_rides' # The variable you are predicting\n",
    "ts_col = 'service_date' # The name of the column with the date field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "\n",
    "freq = 'D' # Daily frequency\n",
    "n_input_steps = 30 # Lookback window\n",
    "n_output_steps = 7 # How many steps to predict forward\n",
    "n_seasons = 7 # Monthly periodicity\n",
    "\n",
    "train_split = 0.8 # % Split between train/test data\n",
    "epochs = 1000 # How many passes through the data (early-stopping will cause training to stop before this)\n",
    "patience = 5 # Terminate training after the validation loss does not decrease after this many epochs\n",
    "\n",
    "lstm_units = 64\n",
    "input_layer_name = 'lstm_input'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "\n",
    "MODEL_NAME = 'cta_ridership'\n",
    "FRAMEWORK='TENSORFLOW'\n",
    "RUNTIME_VERSION = '2.3'\n",
    "PYTHON_VERSION = '3.7'\n",
    "PREDICTIONS_FILE = 'predictions.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "When you submit a training job using the Cloud SDK, you upload a Python package\n",
    "containing your training code to a Cloud Storage bucket. AI Platform runs\n",
    "the code from this package. In this tutorial, AI Platform also saves the\n",
    "trained model that results from your job in the same bucket. You can then\n",
    "create an AI Platform model version based on this output in order to serve\n",
    "online predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "storage_client = storage.Client()\n",
    "try:\n",
    "    bucket = storage_client.get_bucket(BUCKET)\n",
    "    print('Bucket exists, let''s not recreate it.')\n",
    "except:\n",
    "    bucket = storage_client.create_bucket(BUCKET)\n",
    "    print('Created bucket: ' + BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M8_SY9abGxCc"
   },
   "source": [
    "## Load and preview the data\n",
    "\n",
    "Pre-processing on the original dataset has been done for you and made available on Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_file = 'cta_ridership.csv' # Which file to save the results to\n",
    "\n",
    "if os.path.exists(processed_file):\n",
    "    input_file = processed_file # File created in previous lab\n",
    "else:\n",
    "    input_file = f'data/{processed_file}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JOfBsktiGCOp"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(input_file, index_col=ts_col, parse_dates=True)\n",
    "\n",
    "# Plot 30 days of ridership \n",
    "_ = df[target_col][:30].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some characteristics of the data that will be used later\n",
    "n_features = len(df.columns)\n",
    "\n",
    "# Index of target column. Used later when creating dataframes.\n",
    "target_col_num = df.columns.get_loc(target_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "067UQKwlVBUf"
   },
   "source": [
    "### Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 227
    },
    "colab_type": "code",
    "id": "59PwFlYDU13-",
    "outputId": "b4b59f59-6d39-4235-8f9f-8a73d7e1cd0e"
   },
   "outputs": [],
   "source": [
    "# Split data\n",
    "\n",
    "size = int(len(df) * train_split)\n",
    "df_train, df_test = df[0:size].copy(deep=True), df[size:len(df)].copy(deep=True)\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "colab_type": "code",
    "id": "Yn9RSS3DVEtt",
    "outputId": "cd066456-b419-4e0f-8c52-2fd7d82123ba"
   },
   "outputs": [],
   "source": [
    "_ = df_train.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jlgFdwM9VOnd"
   },
   "source": [
    "### Scale values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review original values\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xqRM0Wt6VKzm"
   },
   "outputs": [],
   "source": [
    "# For neural networks to converge quicker, it is helpful to scale the values.\n",
    "# For example, each feature might be transformed to have a mean of 0 and std. dev. of 1.\n",
    "#\n",
    "# You are working with a mix of features, input timesteps, output horizon, etc.\n",
    "# which don't work out-of-the-box with common scaling utilities.\n",
    "# So, here are a couple wrappers to handle scaling and inverting the scaling.\n",
    "\n",
    "feature_scaler = StandardScaler()\n",
    "target_scaler = StandardScaler()\n",
    "\n",
    "def scale(df, \n",
    "          fit=True, \n",
    "          target_col=target_col,\n",
    "          feature_scaler=feature_scaler,\n",
    "          target_scaler=target_scaler):\n",
    "    \"\"\"\n",
    "    Scale the input features, using a separate scaler for the target.\n",
    "    \n",
    "    Parameters: \n",
    "    df (pd.DataFrame): Input dataframe\n",
    "    fit (bool): Whether to fit the scaler to the data (only apply to training data)\n",
    "    target_col (pd.Series): The column that is being predicted\n",
    "    feature_scaler (StandardScaler): Scaler used for features\n",
    "    target_scaler (StandardScaler): Scaler used for target\n",
    "      \n",
    "    Returns: \n",
    "    df_scaled (pd.DataFrame): Scaled dataframe   \n",
    "    \"\"\"    \n",
    "    \n",
    "    target = df[target_col].values.reshape(-1, 1)\n",
    "    if fit:\n",
    "        target_scaler.fit(target)\n",
    "    target_scaled = target_scaler.transform(target)\n",
    "    \n",
    "    # Select all columns other than target to be features\n",
    "    features = df.loc[:, df.columns != target_col].values\n",
    "    \n",
    "    if features.shape[1]:  # If there are any features\n",
    "        if fit:\n",
    "            feature_scaler.fit(features)\n",
    "        features_scaled = feature_scaler.transform(features)\n",
    "        \n",
    "        # Combine target and features into one data frame\n",
    "        df_scaled = pd.DataFrame(features_scaled)\n",
    "        target_col_num = df.columns.get_loc(target_col)\n",
    "        df_scaled.insert(target_col_num, target_col, target_scaled)\n",
    "        df_scaled.columns = df.columns        \n",
    "    \n",
    "    else:  # If only target column (no additional features)\n",
    "        df_scaled = pd.DataFrame(target_scaled, columns=df.columns)\n",
    "      \n",
    "    return df_scaled\n",
    "\n",
    "def inverse_scale(data, target_scaler=target_scaler):\n",
    "    \"\"\"\n",
    "    Transform the scaled values of the target back into their original form.\n",
    "    The features are left alone, as we're assuming that the output of the model only includes the target.\n",
    "    \n",
    "    Parameters: \n",
    "    data (np.array): Input array\n",
    "    target_scaler (StandardScaler): Scaler used for target\n",
    "      \n",
    "    Returns: \n",
    "    data_scaled (np.array): Scaled array   \n",
    "    \"\"\"    \n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    data_scaled = np.empty([data.shape[1], data.shape[0]])\n",
    "    for i in range(data.shape[1]):\n",
    "        data_scaled[i] = target_scaler.inverse_transform(data[:,i])\n",
    "    return data_scaled.transpose()\n",
    "\n",
    "df_train_scaled=scale(df_train)\n",
    "df_test_scaled=scale(df_test, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "4w2K9hVHXV6-",
    "outputId": "b48986d7-e9b4-4a63-c71c-54b43b1ea7ad"
   },
   "outputs": [],
   "source": [
    "# Review scaled values\n",
    "\n",
    "df_train_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CNw7mVozbsA2"
   },
   "source": [
    "### Create sequences of time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "76Wp5gRtbrA9"
   },
   "outputs": [],
   "source": [
    "def reframe(data, n_input_steps = n_input_steps, n_output_steps = n_output_steps, target_col = target_col):\n",
    "\n",
    "    target_col_num = data.columns.get_loc(target_col)    \n",
    "    \n",
    "    # Iterate through data and create sequences of features and outputs\n",
    "    df = pd.DataFrame(data)\n",
    "    cols=list()\n",
    "    for i in range(n_input_steps, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "    for i in range(0, n_output_steps):\n",
    "        cols.append(df.shift(-i))\n",
    "        \n",
    "    # Concatenate values and remove any missing values\n",
    "    df = pd.concat(cols, axis=1)\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # Split the data into feature and target variables\n",
    "    n_feature_cols = n_input_steps * n_features\n",
    "    features = df.iloc[:,0:n_feature_cols]\n",
    "    target_cols = [i for i in range(n_feature_cols + target_col_num, n_feature_cols + n_output_steps * n_features, n_features)]\n",
    "    targets = df.iloc[:,target_cols]\n",
    "\n",
    "    return (features, targets)\n",
    "\n",
    "X_train_reframed, y_train_reframed = reframe(df_train_scaled)\n",
    "X_test_reframed, y_test_reframed = reframe(df_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tqhwqDsxb-E_"
   },
   "source": [
    "## Build a model and submit your training job to AI Platform\n",
    "\n",
    "The model you're building here trains pretty fast so you could train it in this notebook, but for more computationally expensive models, it's useful to train them in the Cloud. To use AI Platform Training, you'll package up your training code and submit a training job to the AI Platform Prediction service.\n",
    "\n",
    "In your training script, you'll also export your trained `SavedModel` to a Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HG1bYOO8b4sN"
   },
   "outputs": [],
   "source": [
    "# Reshape test data to match model inputs and outputs\n",
    "\n",
    "X_train = X_train_reframed.values.reshape(-1, n_input_steps, n_features)\n",
    "X_test = X_test_reframed.values.reshape(-1, n_input_steps, n_features)\n",
    "y_train = y_train_reframed.values.reshape(-1, n_output_steps)\n",
    "y_test = y_test_reframed.values.reshape(-1, n_output_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify directories to be used later\n",
    "\n",
    "TRAINER_DIR = 'trainer'\n",
    "EXPORT_DIR = 'tf_export'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer directory if it doesn't already exist\n",
    "\n",
    "!mkdir $TRAINER_DIR\n",
    "!touch $TRAINER_DIR/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy numpy arrays to npy files\n",
    "\n",
    "np.save(TRAINER_DIR + '/x_train.npy', X_train)\n",
    "np.save(TRAINER_DIR + '/x_test.npy', X_test)\n",
    "np.save(TRAINER_DIR + '/y_train.npy', y_train)\n",
    "np.save(TRAINER_DIR + '/y_test.npy', y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare model code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write training code out to a file that will be submitted to the training job\n",
    "# Note: f-strings are supported in Python 3.6 and above\n",
    "\n",
    "model_template = f\"\"\"import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "from google.cloud import storage\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "n_features = {n_features} # Two features: y (previous values) and whether the date is a holiday\n",
    "n_input_steps = {n_input_steps} # Lookback window\n",
    "n_output_steps = {n_output_steps} # How many steps to predict forward\n",
    "\n",
    "epochs = {epochs} # How many passes through the data (early-stopping will cause training to stop before this)\n",
    "patience = {patience} # Terminate training after the validation loss does not decrease after this many epochs\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--job-dir',\n",
    "        default=None,\n",
    "        help='URL to store the job output')\n",
    "    args = parser.parse_args()\n",
    "    print(args)\n",
    "    return args\n",
    "\n",
    "def main():\n",
    "    args = get_args()\n",
    "    print('args: ', args)\n",
    "    model_dir = args.job_dir\n",
    "    \n",
    "    storage_client = storage.Client()\n",
    "    bucket_name = model_dir.split('/')[2]\n",
    "    \n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "\n",
    "    # Get the training data and convert back to np arrays\n",
    "    local_data_dir = os.path.join(os.getcwd(), tempfile.gettempdir())\n",
    "    \n",
    "    data_files = ['x_train.npy', 'y_train.npy', 'x_test.npy', 'y_test.npy']\n",
    " \n",
    "    for i in data_files:\n",
    "        blob = storage.Blob('{TRAINER_DIR}/' + i, bucket)\n",
    "        destination_file = local_data_dir + '/' + i\n",
    "        open(destination_file, 'a').close()\n",
    "        blob.download_to_filename(destination_file)\n",
    "\n",
    "    X_train = np.load(local_data_dir + '/x_train.npy')\n",
    "    y_train = np.load(local_data_dir + '/y_train.npy')\n",
    "    X_test = np.load(local_data_dir + '/x_test.npy')\n",
    "    y_test = np.load(local_data_dir + '/y_test.npy')\n",
    "    \n",
    "    # Build and train the model\n",
    "    model = Sequential([\n",
    "        LSTM({lstm_units}, input_shape=[n_input_steps, n_features], recurrent_activation=None),\n",
    "        Dense(n_output_steps)])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mae')\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=patience)\n",
    "    _ = model.fit(x=X_train, y=y_train, validation_data=(X_test, y_test), epochs=epochs, callbacks=[early_stopping])\n",
    "    \n",
    "    # Export the model\n",
    "    export_path = os.path.join(model_dir, '{EXPORT_DIR}')\n",
    "    model.save(export_path)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\"\"\"\n",
    "\n",
    "with open(os.path.join(TRAINER_DIR, 'model.py'), 'w') as f:\n",
    "    f.write(model_template.format(**globals()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the model and training data files to a GCS bucket\n",
    "\n",
    "!gsutil -m cp -r trainer $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the contents of the bucket to ensure they were copied properly\n",
    "\n",
    "!gsutil ls $BUCKET_URI/$TRAINER_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run this if you need to create a new training job\n",
    "\n",
    "timestamp = str(datetime.datetime.now().time())\n",
    "JOB_NAME = 'caip_training_' + str(int(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training job parameters\n",
    "\n",
    "MODULE_NAME = TRAINER_DIR + '.model'\n",
    "TRAIN_DIR = os.getcwd() + '/' + TRAINER_DIR\n",
    "JOB_DIR = BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit the training job\n",
    "\n",
    "!gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "        --scale-tier basic \\\n",
    "        --package-path $TRAIN_DIR \\\n",
    "        --module-name $MODULE_NAME \\\n",
    "        --job-dir $BUCKET_URI \\\n",
    "        --region $REGION \\\n",
    "        --runtime-version $RUNTIME_VERSION \\\n",
    "        --python-version $PYTHON_VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the job status\n",
    "\n",
    "!gcloud ai-platform jobs describe $JOB_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor output of your training job\n",
    "\n",
    "Follow the instructions in the output of the gcloud command above to view the logs from your training job. You can also navigate to the [Jobs Section](https://console.cloud.google.com/ai-platform/jobs) of your Cloud Console to view logs.\n",
    "\n",
    "Once your training job completes successfully, it'll export your trained model as a TensorFlow `SavedModel` and write the output to a directory in your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 361
    },
    "colab_type": "code",
    "id": "ECQHhHfUcFoV",
    "outputId": "adefd734-74c3-4041-f39b-fc0c413a7dd7"
   },
   "outputs": [],
   "source": [
    "# Verify model was exported correctly\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.get_bucket(BUCKET)\n",
    "bucket_files = list(bucket.list_blobs(prefix=EXPORT_DIR + '/'))\n",
    "\n",
    "# If you see a saved_model.pb and a variables/ and assets/ directory here, it means your model was exported correctly in your training job. Yay!\n",
    "\n",
    "for file in bucket_files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy a model version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model if it doesn't already exist\n",
    "\n",
    "!gcloud ai-platform models create $MODEL_NAME --regions $REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model version\n",
    "\n",
    "export_path = BUCKET_URI +'/' + EXPORT_DIR\n",
    "version = 'version_' + str(int(time.time()))\n",
    "\n",
    "!gcloud ai-platform versions create $version \\\n",
    "  --model $MODEL_NAME \\\n",
    "  --origin $export_path \\\n",
    "  --runtime-version=$RUNTIME_VERSION \\\n",
    "  --framework $FRAMEWORK \\\n",
    "  --python-version=$PYTHON_VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZFglcq4kcd4R"
   },
   "source": [
    "## Get predictions on deployed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize client\n",
    "\n",
    "endpoint = f'https://ml.googleapis.com'\n",
    "client_options = ClientOptions(api_endpoint=endpoint)\n",
    "service = googleapiclient.discovery.build('ml', 'v1', client_options=client_options, cache_discovery=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to invoke the prediction service from\n",
    "# https://github.com/GoogleCloudPlatform/python-docs-samples/blob/master/ml_engine/online_prediction/predict.py\n",
    "\n",
    "def predict_json(project, model, instances, version=None):\n",
    "    \"\"\"Send json data to a deployed model for prediction.\n",
    "    Args:\n",
    "        project (str): project where the AI Platform Model is deployed.\n",
    "        model (str): model name.\n",
    "        instances ([Mapping[str: Any]]): Keys should be the names of Tensors\n",
    "            your deployed model expects as inputs. Values should be datatypes\n",
    "            convertible to Tensors, or (potentially nested) lists of datatypes\n",
    "            convertible to tensors.\n",
    "        version: str, version of the model to target.\n",
    "    Returns:\n",
    "        Mapping[str: any]: dictionary of prediction results defined by the\n",
    "            model.\n",
    "    \"\"\"\n",
    "\n",
    "    name = 'projects/{}/models/{}'.format(project, model)\n",
    "\n",
    "    if version is not None:\n",
    "        name += '/versions/{}'.format(version)\n",
    "\n",
    "    response = service.projects().predict(\n",
    "        name=name,\n",
    "        body={'instances': instances}\n",
    "    ).execute()\n",
    "\n",
    "    if 'error' in response:\n",
    "        raise RuntimeError(response['error'])\n",
    "\n",
    "    return response['predictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with the 1st element from the test set\n",
    "\n",
    "prediction_json = {input_layer_name: X_test[0].tolist()}\n",
    "\n",
    "pred_val = predict_json(PROJECT, MODEL_NAME, prediction_json)\n",
    "\n",
    "pred_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print prediction and compare to actual value\n",
    "\n",
    "print('Predicted riders:', int(round(inverse_scale(np.array([pred_val[0]['dense'][0]]).reshape(1,1))[0][0])))\n",
    "print('Actual riders:   ', int(round(inverse_scale(np.array([y_test[0]]))[0][0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete model version resource\n",
    "!echo gcloud ai-platform versions delete {version} --model {MODEL_NAME} --quiet \n",
    "\n",
    "# Delete model resource\n",
    "!echo gcloud ai-platform models delete {MODEL_NAME} --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you've learned how to:\n",
    "* Prepare data and models for training in the cloud\n",
    "* Train your model and monitor the progress of the job with AI Platform Training\n",
    "* Predict using the model with AI Platform Predictions"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "liquor-sales-xai.ipynb",
   "provenance": []
  },
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
